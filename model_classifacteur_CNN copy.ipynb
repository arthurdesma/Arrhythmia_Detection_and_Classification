{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fusionné = pd.read_csv('df_fusionné.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fusionné_binaire = df_fusionné.sample(frac=0.01)\n",
    "df_fusionné_binaire.iloc[:, 0] = df_fusionné_binaire.iloc[:, 0].apply(lambda x: 0 if x == 'N' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_fusionné_binaire.iloc[:, 0].value_counts()\n",
    "min_count = counts.min()\n",
    "\n",
    "# Create balanced DataFrame through undersampling\n",
    "df_balanced_under = pd.concat([\n",
    "    df_fusionné_binaire[df_fusionné_binaire.iloc[:, 0] == 0].sample(min_count),\n",
    "    df_fusionné_binaire[df_fusionné_binaire.iloc[:, 0] == 1].sample(min_count)\n",
    "])\n",
    "\n",
    "# Shuffle the DataFrame to mix the classes\n",
    "df_balanced_under = df_balanced_under.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnName\n",
      "1    378\n",
      "0    378\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "value_counts = df_balanced_under.iloc[:, 0].value_counts()\n",
    "\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ColumnName</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195280</th>\n",
       "      <td>1</td>\n",
       "      <td>0.738967</td>\n",
       "      <td>0.736213</td>\n",
       "      <td>0.724819</td>\n",
       "      <td>0.707381</td>\n",
       "      <td>0.701063</td>\n",
       "      <td>0.695575</td>\n",
       "      <td>0.671175</td>\n",
       "      <td>0.640686</td>\n",
       "      <td>0.618701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462469</td>\n",
       "      <td>0.459836</td>\n",
       "      <td>0.462348</td>\n",
       "      <td>0.465820</td>\n",
       "      <td>0.468463</td>\n",
       "      <td>0.474832</td>\n",
       "      <td>0.463613</td>\n",
       "      <td>0.467341</td>\n",
       "      <td>0.486698</td>\n",
       "      <td>0.470600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13104</th>\n",
       "      <td>0</td>\n",
       "      <td>0.470099</td>\n",
       "      <td>0.464199</td>\n",
       "      <td>0.458407</td>\n",
       "      <td>0.452593</td>\n",
       "      <td>0.452615</td>\n",
       "      <td>0.454657</td>\n",
       "      <td>0.457487</td>\n",
       "      <td>0.460664</td>\n",
       "      <td>0.461092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650538</td>\n",
       "      <td>0.597622</td>\n",
       "      <td>0.536399</td>\n",
       "      <td>0.475443</td>\n",
       "      <td>0.456459</td>\n",
       "      <td>0.454757</td>\n",
       "      <td>0.449382</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.457477</td>\n",
       "      <td>0.471592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83763</th>\n",
       "      <td>0</td>\n",
       "      <td>0.432122</td>\n",
       "      <td>0.434323</td>\n",
       "      <td>0.436297</td>\n",
       "      <td>0.437908</td>\n",
       "      <td>0.438831</td>\n",
       "      <td>0.439154</td>\n",
       "      <td>0.438237</td>\n",
       "      <td>0.436536</td>\n",
       "      <td>0.434902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416860</td>\n",
       "      <td>0.414766</td>\n",
       "      <td>0.415939</td>\n",
       "      <td>0.418293</td>\n",
       "      <td>0.421071</td>\n",
       "      <td>0.424099</td>\n",
       "      <td>0.425970</td>\n",
       "      <td>0.427515</td>\n",
       "      <td>0.428829</td>\n",
       "      <td>0.430048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149480</th>\n",
       "      <td>1</td>\n",
       "      <td>0.503518</td>\n",
       "      <td>0.501392</td>\n",
       "      <td>0.508724</td>\n",
       "      <td>0.497203</td>\n",
       "      <td>0.501599</td>\n",
       "      <td>0.506959</td>\n",
       "      <td>0.504784</td>\n",
       "      <td>0.500468</td>\n",
       "      <td>0.500554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399701</td>\n",
       "      <td>0.403151</td>\n",
       "      <td>0.401560</td>\n",
       "      <td>0.398480</td>\n",
       "      <td>0.401670</td>\n",
       "      <td>0.407747</td>\n",
       "      <td>0.404729</td>\n",
       "      <td>0.406546</td>\n",
       "      <td>0.412507</td>\n",
       "      <td>0.405883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113900</th>\n",
       "      <td>1</td>\n",
       "      <td>0.466519</td>\n",
       "      <td>0.470302</td>\n",
       "      <td>0.459624</td>\n",
       "      <td>0.469558</td>\n",
       "      <td>0.464893</td>\n",
       "      <td>0.460021</td>\n",
       "      <td>0.463678</td>\n",
       "      <td>0.469627</td>\n",
       "      <td>0.468748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592582</td>\n",
       "      <td>0.590206</td>\n",
       "      <td>0.592808</td>\n",
       "      <td>0.596506</td>\n",
       "      <td>0.593301</td>\n",
       "      <td>0.586388</td>\n",
       "      <td>0.588057</td>\n",
       "      <td>0.583756</td>\n",
       "      <td>0.573306</td>\n",
       "      <td>0.576623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166159</th>\n",
       "      <td>1</td>\n",
       "      <td>0.502682</td>\n",
       "      <td>0.498059</td>\n",
       "      <td>0.505445</td>\n",
       "      <td>0.496076</td>\n",
       "      <td>0.499837</td>\n",
       "      <td>0.504057</td>\n",
       "      <td>0.499835</td>\n",
       "      <td>0.493254</td>\n",
       "      <td>0.492678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435302</td>\n",
       "      <td>0.440561</td>\n",
       "      <td>0.436406</td>\n",
       "      <td>0.428850</td>\n",
       "      <td>0.427526</td>\n",
       "      <td>0.428909</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.415223</td>\n",
       "      <td>0.419644</td>\n",
       "      <td>0.414298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172549</th>\n",
       "      <td>0</td>\n",
       "      <td>0.496979</td>\n",
       "      <td>0.495891</td>\n",
       "      <td>0.495426</td>\n",
       "      <td>0.495294</td>\n",
       "      <td>0.494434</td>\n",
       "      <td>0.493699</td>\n",
       "      <td>0.492969</td>\n",
       "      <td>0.492680</td>\n",
       "      <td>0.494586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517238</td>\n",
       "      <td>0.518530</td>\n",
       "      <td>0.517401</td>\n",
       "      <td>0.515201</td>\n",
       "      <td>0.512745</td>\n",
       "      <td>0.509936</td>\n",
       "      <td>0.507163</td>\n",
       "      <td>0.504460</td>\n",
       "      <td>0.501447</td>\n",
       "      <td>0.498605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138642</th>\n",
       "      <td>0</td>\n",
       "      <td>0.482342</td>\n",
       "      <td>0.481324</td>\n",
       "      <td>0.479470</td>\n",
       "      <td>0.478142</td>\n",
       "      <td>0.482166</td>\n",
       "      <td>0.488348</td>\n",
       "      <td>0.495548</td>\n",
       "      <td>0.503326</td>\n",
       "      <td>0.509591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474259</td>\n",
       "      <td>0.474885</td>\n",
       "      <td>0.474996</td>\n",
       "      <td>0.475058</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.474528</td>\n",
       "      <td>0.475715</td>\n",
       "      <td>0.477231</td>\n",
       "      <td>0.479960</td>\n",
       "      <td>0.482770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101255</th>\n",
       "      <td>0</td>\n",
       "      <td>0.485836</td>\n",
       "      <td>0.486327</td>\n",
       "      <td>0.486685</td>\n",
       "      <td>0.487057</td>\n",
       "      <td>0.487750</td>\n",
       "      <td>0.488435</td>\n",
       "      <td>0.489231</td>\n",
       "      <td>0.489882</td>\n",
       "      <td>0.489480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.484871</td>\n",
       "      <td>0.484917</td>\n",
       "      <td>0.485007</td>\n",
       "      <td>0.485178</td>\n",
       "      <td>0.485373</td>\n",
       "      <td>0.485415</td>\n",
       "      <td>0.485443</td>\n",
       "      <td>0.485451</td>\n",
       "      <td>0.485491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75016</th>\n",
       "      <td>0</td>\n",
       "      <td>0.432665</td>\n",
       "      <td>0.432253</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.431344</td>\n",
       "      <td>0.430842</td>\n",
       "      <td>0.430335</td>\n",
       "      <td>0.429791</td>\n",
       "      <td>0.429260</td>\n",
       "      <td>0.428874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433407</td>\n",
       "      <td>0.433427</td>\n",
       "      <td>0.433426</td>\n",
       "      <td>0.433407</td>\n",
       "      <td>0.433338</td>\n",
       "      <td>0.433247</td>\n",
       "      <td>0.433202</td>\n",
       "      <td>0.433138</td>\n",
       "      <td>0.433078</td>\n",
       "      <td>0.432976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 361 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ColumnName         0         1         2         3         4         5  \\\n",
       "195280          1  0.738967  0.736213  0.724819  0.707381  0.701063  0.695575   \n",
       "13104           0  0.470099  0.464199  0.458407  0.452593  0.452615  0.454657   \n",
       "83763           0  0.432122  0.434323  0.436297  0.437908  0.438831  0.439154   \n",
       "149480          1  0.503518  0.501392  0.508724  0.497203  0.501599  0.506959   \n",
       "113900          1  0.466519  0.470302  0.459624  0.469558  0.464893  0.460021   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "166159          1  0.502682  0.498059  0.505445  0.496076  0.499837  0.504057   \n",
       "172549          0  0.496979  0.495891  0.495426  0.495294  0.494434  0.493699   \n",
       "138642          0  0.482342  0.481324  0.479470  0.478142  0.482166  0.488348   \n",
       "101255          0  0.485836  0.486327  0.486685  0.487057  0.487750  0.488435   \n",
       "75016           0  0.432665  0.432253  0.431818  0.431344  0.430842  0.430335   \n",
       "\n",
       "               6         7         8  ...       350       351       352  \\\n",
       "195280  0.671175  0.640686  0.618701  ...  0.462469  0.459836  0.462348   \n",
       "13104   0.457487  0.460664  0.461092  ...  0.650538  0.597622  0.536399   \n",
       "83763   0.438237  0.436536  0.434902  ...  0.416860  0.414766  0.415939   \n",
       "149480  0.504784  0.500468  0.500554  ...  0.399701  0.403151  0.401560   \n",
       "113900  0.463678  0.469627  0.468748  ...  0.592582  0.590206  0.592808   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "166159  0.499835  0.493254  0.492678  ...  0.435302  0.440561  0.436406   \n",
       "172549  0.492969  0.492680  0.494586  ...  0.517238  0.518530  0.517401   \n",
       "138642  0.495548  0.503326  0.509591  ...  0.474259  0.474885  0.474996   \n",
       "101255  0.489231  0.489882  0.489480  ...  0.484940  0.484871  0.484917   \n",
       "75016   0.429791  0.429260  0.428874  ...  0.433407  0.433427  0.433426   \n",
       "\n",
       "             353       354       355       356       357       358       359  \n",
       "195280  0.465820  0.468463  0.474832  0.463613  0.467341  0.486698  0.470600  \n",
       "13104   0.475443  0.456459  0.454757  0.449382  0.446810  0.457477  0.471592  \n",
       "83763   0.418293  0.421071  0.424099  0.425970  0.427515  0.428829  0.430048  \n",
       "149480  0.398480  0.401670  0.407747  0.404729  0.406546  0.412507  0.405883  \n",
       "113900  0.596506  0.593301  0.586388  0.588057  0.583756  0.573306  0.576623  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "166159  0.428850  0.427526  0.428909  0.419919  0.415223  0.419644  0.414298  \n",
       "172549  0.515201  0.512745  0.509936  0.507163  0.504460  0.501447  0.498605  \n",
       "138642  0.475058  0.474727  0.474528  0.475715  0.477231  0.479960  0.482770  \n",
       "101255  0.485007  0.485178  0.485373  0.485415  0.485443  0.485451  0.485491  \n",
       "75016   0.433407  0.433338  0.433247  0.433202  0.433138  0.433078  0.432976  \n",
       "\n",
       "[756 rows x 361 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_balanced_under.iloc[:, 1:]  \n",
    "y = df_balanced_under.iloc[:, 0] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam, Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model building function\n",
    "def build_model(learning_rate=0.001):\n",
    "\n",
    "    Model = Sequential()\n",
    "\n",
    "    Model.add(Conv1D(512, 20, strides=1, padding=\"same\", activation=\"relu\", input_shape=(360, 1)))\n",
    "    Model.add(BatchNormalization())\n",
    "    Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "\n",
    "    Model.add(Conv1D(256, 20, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    Model.add(Dropout(0.2))\n",
    "    Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "\n",
    "    Model.add(Conv1D(1024, 20, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    Model.add(Dropout(0.2))\n",
    "    Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "\n",
    "    Model.add(Conv1D(128, 20, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    Model.add(Dropout(0.2))\n",
    "    Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "\n",
    "    Model.add(Conv1D(256, 20, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    Model.add(Dropout(0.2))\n",
    "    Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "\n",
    "    Model.add(Conv1D(64, 20, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    Model.add(Dropout(0.2))\n",
    "    Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "\n",
    "    Model.add(Conv1D(32, 20, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    Model.add(Dropout(0.2))\n",
    "    Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "\n",
    "    Model.add(Flatten())\n",
    "    Model.add(Dense(1024, activation='relu'))\n",
    "    Model.add(Dropout(0.2))\n",
    "\n",
    "    Model.add(Dense(512, activation='relu'))\n",
    "    Model.add(Dropout(0.2))\n",
    "\n",
    "    Model.add(Dense(64, activation='relu'))\n",
    "    Model.add(Dropout(0.2))\n",
    "\n",
    "    Model.add(Dense(units=1, activation='sigmoid'))  # Use 'sigmoid' for binary classification\n",
    "    \n",
    "    Model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Wrap the model with KerasClassifier (move the epochs parameter to the fit method)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m KerasClassifier(model\u001b[38;5;241m=\u001b[39mbuild_model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m# Default epochs value\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "# Wrap the model with KerasClassifier (move the epochs parameter to the fit method)\n",
    "model = KerasClassifier(model=build_model, verbose=3, epochs=5) # Default epochs value\n",
    "\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.001],\n",
    "    'epochs': [1]\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=2)\n",
    "grid_result = grid.fit(X_train, y_train) # Assuming you have defined X_train and y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for mean_score, params in zip(grid_result.cv_results_['mean_test_score'], grid_result.cv_results_['params']):\n",
    "    print(\"%f with: %r\" % (mean_score, params))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
